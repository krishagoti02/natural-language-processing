{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Krisha Goti C035"
      ],
      "metadata": {
        "id": "suct3GTPZUqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B.1 Tasks given in PART A to be completed here**\n",
        "\n"
      ],
      "metadata": {
        "id": "sGMzzHLFZXNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text representation using pre-trained word embedding models."
      ],
      "metadata": {
        "id": "C4N1rOAcLvz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeJjxe4BLgeS",
        "outputId": "8942e813-e4b2-492f-bb38-2c7b8b224eb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-20 14:15:56--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-02-20 14:15:57--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-02-20 14:15:57--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.99MB/s    in 2m 40s  \n",
            "\n",
            "2023-02-20 14:18:37 (5.15 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "x = {'text', 'the', 'leader', 'prime',\n",
        "\t'natural', 'language'}\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x)\n",
        "\n",
        "\n",
        "print(\"Number of unique words in dictionary=\",\n",
        "\tlen(tokenizer.word_index))\n",
        "print(\"Dictionary is = \", tokenizer.word_index)\n",
        "\n",
        "\n",
        "def embedding_for_vocab(filepath, word_index, embedding_dim):\n",
        "\tvocab_size = len(word_index) + 1\n",
        "\n",
        "\tembedding_matrix_vocab = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "\twith open(filepath, encoding=\"utf8\") as f:\n",
        "\t\tfor line in f:\n",
        "\t\t\tword, *vector = line.split()\n",
        "\t\t\tif word in word_index:\n",
        "\t\t\t\tidx = word_index[word]\n",
        "\t\t\t\tembedding_matrix_vocab[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "\treturn embedding_matrix_vocab\n",
        "\n",
        "\n",
        "embedding_dim = 50\n",
        "embedding_matrix_vocab = embedding_for_vocab('glove.6B.50d.txt', tokenizer.word_index,embedding_dim)\n",
        "\n",
        "print(\"Dense vector for first word is => \",\n",
        "\tembedding_matrix_vocab[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ahu-l4VQLgbM",
        "outputId": "01ee7f48-ca23-497f-94e2-bff80ba7d23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in dictionary= 6\n",
            "Dictionary is =  {'text': 1, 'prime': 2, 'natural': 3, 'the': 4, 'leader': 5, 'language': 6}\n",
            "Dense vector for first word is =>  [ 0.32615     0.36686    -0.0074905  -0.37553     0.66715002  0.21646\n",
            " -0.19801    -1.10010004 -0.42221001  0.10574    -0.31292     0.50953001\n",
            "  0.55774999  0.12019     0.31441    -0.25042999 -1.06369996 -1.32130003\n",
            "  0.87797999 -0.24627     0.27379    -0.51091999  0.49324     0.52243\n",
            "  1.16359997 -0.75322998 -0.48052999 -0.11259    -0.54595    -0.83920997\n",
            "  2.98250008 -1.19159997 -0.51958001 -0.39365    -0.1419     -0.026977\n",
            "  0.66295999  0.16574    -1.1681      0.14443     1.63049996 -0.17216\n",
            " -0.17436001 -0.01049    -0.17794     0.93076003  1.0381      0.94265997\n",
            " -0.14805    -0.61109   ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training embedding’s using gensim"
      ],
      "metadata": {
        "id": "xDiXNG61LyV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "import multiprocessing\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "d3Jq8pSJt6gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://en.wikipedia.org/wiki/Lionel_Messi\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "first=soup.find_all('p')\n",
        "data1=first[7].text\n",
        "\n",
        "df = pd.DataFrame(data1.split(\" \"),columns=['words'])\n",
        "\n",
        "\n",
        "data2, data3, data4 = [], [], []\n",
        "\n",
        "work = ''\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "bow = vectorizer.fit_transform(df['words'])\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(feature_names)\n",
        "\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(sentences=common_texts, size=100, window=5, min_count=1, workers=4)\n",
        "model.save(\"word2vec.model\")\n",
        "\n",
        "\n",
        "model1 = Word2Vec(feature_names, min_count = 1,size = 100, window = 5)\n",
        "\n",
        "model2 = Word2Vec(feature_names, min_count = 1, size = 100, window = 5, sg = 1)\n",
        "\n",
        "model2.wv.most_similar('goals','messi')\n",
        "\n",
        "import gensim.downloader\n",
        "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
        "glove_vectors.most_similar('twitter')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxKAIEEbt-fS",
        "outputId": "60f118d1-3a73-4a3e-b18d-edec26c7a326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['09' '12' '13' '15' '17' '2004' '2008' '2011' '2014' '2015' '2018' '2019'\n",
            " '2021' '22' 'achieve' 'after' 'age' 'aged' 'all' 'an' 'and' 'argentina'\n",
            " 'as' 'assumed' 'at' 'august' 'award' 'awarded' 'ballon' 'ballons'\n",
            " 'barcelona' 'becoming' 'before' 'behind' 'best' 'born' 'campaign'\n",
            " 'captaincy' 'career' 'central' 'club' 'competitive' 'consecutive'\n",
            " 'contract' 'cristiano' 'debut' 'during' 'established' 'establishing'\n",
            " 'european' 'fifth' 'finished' 'first' 'followed' 'following' 'football'\n",
            " 'for' 'form' 'four' 'germain' 'goals' 'he' 'helped' 'him' 'himself' 'his'\n",
            " 'historic' 'in' 'integral' 'join' 'la' 'leading' 'liga' 'made' 'making'\n",
            " 'messi' 'most' 'next' 'october' 'of' 'or' 'out' 'paris' 'perceived'\n",
            " 'player' 'raised' 'record' 'records' 'regaining' 'relocated' 'rival'\n",
            " 'ronaldo' 'saint' 'scored' 'scorer' 'season' 'seasons' 'second' 'set'\n",
            " 'signed' 'single' 'sixth' 'spain' 'spanish' 'successful' 'that' 'the'\n",
            " 'three' 'time' 'times' 'to' 'top' 'treble' 'two' 'uninterrupted' 'was'\n",
            " 'which' 'while' 'whom' 'win' 'winning' 'with' 'within' 'won' 'year'\n",
            " 'years']\n",
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('facebook', 0.9480051398277283),\n",
              " ('tweet', 0.9403422474861145),\n",
              " ('fb', 0.9342358708381653),\n",
              " ('instagram', 0.9104823470115662),\n",
              " ('chat', 0.8964964747428894),\n",
              " ('hashtag', 0.8885936141014099),\n",
              " ('tweets', 0.8878157734870911),\n",
              " ('tl', 0.8778461813926697),\n",
              " ('link', 0.877821147441864),\n",
              " ('internet', 0.8753897547721863)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B.2 Observations and Learning:**"
      ],
      "metadata": {
        "id": "Kn4STgPiZqGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this experiment we have observed and studied about\n",
        "a) Text representation using pre-trained word embedding models.\n",
        "b) Training embedding’s using gensim\n"
      ],
      "metadata": {
        "id": "Oc11_HoyZcnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B.3 Conclusion:**"
      ],
      "metadata": {
        "id": "_nq7g3GuZsym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After successfully completing this experiment we are able to:\n",
        "1) Understand Text representation using pre-trained word embedding models\n",
        "2) Design and Implementation of training embeddings using Gensim\n",
        "\n"
      ],
      "metadata": {
        "id": "zqromqYhZtRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B.4 Question of curiosity:**\n"
      ],
      "metadata": {
        "id": "LNoRd640Z3-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What are basic ways of word representation in NLP?**\n",
        "\n",
        "There are several basic ways of word representation in NLP, including:\n",
        "\n",
        "a) One-Hot Encoding: Each word is represented as a vector of 0s and 1s, where the vector is the size of the entire vocabulary and only one entry is 1, representing the word. This is a very basic approach but suffers from the curse of dimensionality when the vocabulary is very large.\n",
        "Bag of Words: Each word is represented as a vector of counts, where the vector is the size of the entire vocabulary and each entry represents the count of the corresponding word in the document. This approach is also simple but discards the ordering of the words in the document.\n",
        "\n",
        "b) TF-IDF: This method is similar to the Bag of Words approach, but the counts are weighted by the inverse document frequency of the word, which gives more importance to rare words that appear in a specific document.\n",
        "\n",
        "c) Word Embeddings: Word embeddings represent each word as a dense vector of continuous values, where similar words are represented by vectors that are close in the vector space. Word embeddings are learned using neural networks and have become the dominant approach for word representation in recent years.\n",
        "\n",
        "d) Subword Encoding: In this approach, words are broken down into subword units, which are then represented using one of the above methods. This approach can help capture morphological information and handle out-of-vocabulary words. Examples of subword encoding methods include Byte Pair Encoding (BPE) and WordPiece.\n",
        "\n",
        "\n",
        "**2. What is the importance of pre-trained word embeddings?**\n",
        "\n",
        "Pre-trained word embeddings are pre-computed representations of words that have been learned from large amounts of text data. These embeddings can be downloaded and used directly in many NLP tasks without the need to train new embeddings on the specific task or data at hand.\n",
        "\n",
        "The importance of pre-trained word embeddings is that they can provide several benefits to NLP models, such as:\n",
        "\n",
        "a) Reduced training time: Training high-quality word embeddings from scratch can be a computationally expensive process that requires large amounts of text data. Pre-trained word embeddings can be used directly, reducing the need for training from scratch, and therefore reducing the training time.\n",
        "\n",
        "b) Improved generalization: Pre-trained word embeddings are trained on large and diverse datasets, allowing them to capture general linguistic patterns and relationships that can be useful across many NLP tasks. This can help models generalize better to new and unseen data.\n",
        "\n",
        "c) Improved performance: Pre-trained word embeddings have been shown to improve the performance of many NLP models, such as text classification, named entity recognition, and machine translation, among others.\n",
        "\n",
        "d) Handling out-of-vocabulary words: Pre-trained word embeddings can help handle words that are not present in the training data, or that are misspelled or abbreviated. By using subword information or context from nearby words, pre-trained embeddings can provide meaningful representations for previously unseen words.\n",
        "\n",
        "In summary, pre-trained word embeddings are an important component of many NLP models, providing benefits such as reduced training time, improved generalization, and better performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**3. Explain popular types of pretrained word embeddings – Word2Vec and GloVe.**\n",
        "\n",
        "Word2Vec and GloVe are two popular types of pre-trained word embeddings. Here is a brief explanation of each:\n",
        "\n",
        "* Word2Vec: Word2Vec is a family of models for learning word embeddings from raw text data. Two of the most common Word2Vec models are Continuous Bag-of-Words (CBOW) and Skip-Gram. CBOW predicts a target word given its context words, while Skip-Gram predicts context words given a target word. The output of these models is a set of dense vectors that represent each word in the vocabulary. Word2Vec is trained using a neural network and is based on the assumption that words that appear in similar contexts have similar meanings.\n",
        "* GloVe: GloVe (Global Vectors for Word Representation) is another approach for learning word embeddings from raw text data. Unlike Word2Vec, GloVe uses global word co-occurrence statistics to learn the embeddings. The idea behind GloVe is that word co-occurrence can be represented as a matrix, which can then be factorized to obtain the embeddings. GloVe is based on the assumption that words that co-occur frequently have similar meanings.\n",
        "\n",
        "**4. Compare and contrast pre trained word embeddings and learning embeddings from scratch.**\n",
        "\n",
        "Pretrained word embeddings and learning embeddings from scratch are two different approaches to representing words as dense vectors in NLP models. Here is a comparison of the two approaches:\n",
        "\n",
        "* Pre Trained word embeddings:\n",
        "\n",
        "a) Pre-trained word embeddings are learned using large amounts of data and are available for download and use in many NLP tasks.\n",
        "\n",
        "b) Pretrained embeddings are trained using sophisticated algorithms and are designed to capture semantic and syntactic relationships between words.\n",
        "\n",
        "c) Pretrained embeddings can reduce the need for training new embeddings from scratch, which can be computationally expensive.\n",
        "\n",
        "d) Pretrained embeddings can provide better performance on many NLP tasks due to their generalizability.\n",
        "\n",
        "* Learning embeddings from scratch:\n",
        "\n",
        "a) Learning embeddings from scratch requires training on a specific task or dataset and can be computationally expensive.\n",
        "\n",
        "b) Learning embeddings from scratch can provide more personalized embeddings that are tailored to the specific task or dataset.\n",
        "\n",
        "c) Learning embeddings from scratch can be useful when working with specialized domains or languages that may not have pre-trained embeddings available.\n",
        "\n",
        "d) Learning embeddings from scratch may require more data than pretrained embeddings to learn high-quality embeddings.\n",
        "\n"
      ],
      "metadata": {
        "id": "hBErb4zSZ4q_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}